
# conda environments:
#
base                   /remote-home1/jqchen/miniconda3
llm118                 /remote-home1/jqchen/miniconda3/envs/llm118
llm118_debug           /remote-home1/jqchen/miniconda3/envs/llm118_debug
llm118_new             /remote-home1/jqchen/miniconda3/envs/llm118_new
new_stb                /remote-home1/jqchen/miniconda3/envs/new_stb
test                   /remote-home1/jqchen/miniconda3/envs/test
th_r                   /remote-home1/jqchen/miniconda3/envs/th_r
toolhall               /remote-home1/jqchen/miniconda3/envs/toolhall
toolhall_2             /remote-home1/jqchen/miniconda3/envs/toolhall_2
toolhall_clean         /remote-home1/jqchen/miniconda3/envs/toolhall_clean
toolhall_copy          /remote-home1/jqchen/miniconda3/envs/toolhall_copy
toolhall_require       /remote-home1/jqchen/miniconda3/envs/toolhall_require
vllm0.7              * /remote-home1/jqchen/miniconda3/envs/vllm0.7

/remote-home1/jqchen/miniconda3/envs/vllm0.7/bin/python
=== ç¯å¢ƒä¿¡æ¯æ£€æŸ¥ ===

vLLM ç‰ˆæœ¬: 0.7.3

=== vLLM å¼•æ“åˆå§‹åŒ– ===
æ­£åœ¨åˆå§‹åŒ– vLLM å¼•æ“...
æ³¨æ„: vLLM åˆå§‹åŒ–å¯èƒ½éœ€è¦å‡ åˆ†é’Ÿæ—¶é—´
æ­£åœ¨åŠ è½½åˆ†è¯å™¨...
vLLM å¼•æ“å’Œåˆ†è¯å™¨åˆå§‹åŒ–å®Œæˆï¼

INFO 07-07 10:58:02 __init__.py:207] Automatically detected platform cuda.
INFO 07-07 10:58:08 config.py:549] This model supports multiple tasks: {'embed', 'score', 'generate', 'reward', 'classify'}. Defaulting to 'generate'.
WARNING 07-07 10:58:08 cuda.py:95] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used
WARNING 07-07 10:58:08 config.py:685] Async output processing is not supported on the current platform type cuda.
INFO 07-07 10:58:08 llm_engine.py:234] Initializing a V0 LLM engine (v0.7.3) with config: model='/remote-home1/share/models/Qwen3-8B', speculative_config=None, tokenizer='/remote-home1/share/models/Qwen3-8B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=/remote-home1/share/models/Qwen3-8B, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=False, chunked_prefill_enabled=False, use_async_output_proc=False, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[],"max_capture_size":0}, use_cached_outputs=False, 
INFO 07-07 10:58:09 cuda.py:229] Using Flash Attention backend.
INFO 07-07 10:58:09 model_runner.py:1110] Starting to load model /remote-home1/share/models/Qwen3-8B...
WARNING 07-07 10:58:09 utils.py:78] Qwen3ForCausalLM has no vLLM implementation, falling back to Transformers implementation. Some features may not be supported and performance may not be optimal.
INFO 07-07 10:58:09 transformers.py:129] Using Transformers backend.
Loading safetensors checkpoint shards:   0% Completed | 0/5 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  20% Completed | 1/5 [00:00<00:02,  1.92it/s]
Loading safetensors checkpoint shards:  40% Completed | 2/5 [00:01<00:03,  1.01s/it]
Loading safetensors checkpoint shards:  60% Completed | 3/5 [00:03<00:02,  1.33s/it]
Loading safetensors checkpoint shards:  80% Completed | 4/5 [00:05<00:01,  1.48s/it]
Loading safetensors checkpoint shards: 100% Completed | 5/5 [00:07<00:00,  1.57s/it]
Loading safetensors checkpoint shards: 100% Completed | 5/5 [00:07<00:00,  1.40s/it]

INFO 07-07 10:58:17 model_runner.py:1115] Loading model weights took 15.2585 GB
INFO 07-07 10:58:19 worker.py:267] Memory profiling takes 1.04 seconds
INFO 07-07 10:58:19 worker.py:267] the current vLLM instance can use total_gpu_memory (23.55GiB) x gpu_memory_utilization (0.80) = 18.84GiB
INFO 07-07 10:58:19 worker.py:267] model weights take 15.26GiB; non_torch_memory takes 0.07GiB; PyTorch activation peak memory takes 1.42GiB; the rest of the memory reserved for KV Cache is 2.09GiB.
INFO 07-07 10:58:19 executor_base.py:111] # cuda blocks: 951, # CPU blocks: 1820
INFO 07-07 10:58:19 executor_base.py:116] Maximum concurrency for 4096 tokens per request: 3.71x
INFO 07-07 10:58:20 llm_engine.py:436] init engine (profile, create kv cache, warmup model) took 3.07 seconds
=== æµ‹è¯•1: æ™®é€šå¯¹è¯æ¨¡å¼ (vLLM) ===
è¾“å…¥æ–‡æœ¬: <|im_start|>system
ä½ æ˜¯ä¸€ä¸ªä¼šæ·±æ€ç†Ÿè™‘çš„AIåŠ©æ‰‹ã€‚<|im_end|>
<|im_start|>user
ä½ å¥½ï¼Œæˆ‘æ˜¯é‚±é”¡é¹è€å¸ˆçš„å­¦ç”Ÿã€‚<|im_end|>
<|im_start|>assistant
<think>

</think>



å¼€å§‹ç”Ÿæˆå›å¤ (vLLM)...
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:02<00:00,  2.01s/it, est. speed input: 18.45 toks/s, output: 33.91 toks/s]Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:03<00:00,  3.04s/it, est. speed input: 18.45 toks/s, output: 33.91 toks/s]
AIå›å¤ï¼ˆæ™®é€šæ¨¡å¼ - vLLMï¼‰:
============================================================
ä½ å¥½ï¼Œå¾ˆé«˜å…´è®¤è¯†ä½ ï¼æˆ‘æ˜¯é‚±é”¡é¹è€å¸ˆçš„å­¦ç”Ÿï¼Œéå¸¸è£å¹¸èƒ½å’Œä½ äº¤æµã€‚é‚±è€å¸ˆåœ¨è‡ªç„¶è¯­è¨€å¤„ç†é¢†åŸŸæœ‰ç€æ·±åšçš„é€ è¯£ï¼Œä»–çš„ç ”ç©¶å¯¹æˆ‘ä»¬è¿™ä¸€ä»£å­¦è€…å½±å“æ·±è¿œã€‚ä½ å¯¹ä»–çš„ç ”ç©¶æœ‰ä»€ä¹ˆç‰¹åˆ«æ„Ÿå…´è¶£çš„æ–¹å‘å—ï¼Ÿæˆ–è€…ä½ æœ€è¿‘åœ¨å­¦ä¹ æˆ–ç ”ç©¶ä¸­é‡åˆ°äº†ä»€ä¹ˆé—®é¢˜ï¼Œéœ€è¦è®¨è®ºå—ï¼Ÿ
============================================================
æ¨ç†è€—æ—¶: 3.05 ç§’

==================================================

=== æµ‹è¯•2: æ€ç»´é“¾æ¨ç†æ¨¡å¼ (vLLM) ===
è¾“å…¥æ–‡æœ¬: <|im_start|>system
ä½ æ˜¯ä¸€ä¸ªä¼šæ·±æ€ç†Ÿè™‘çš„AIåŠ©æ‰‹ã€‚<|im_end|>
<|im_start|>user
ä½ å¥½ï¼Œæˆ‘æ˜¯é‚±é”¡é¹è€å¸ˆçš„å­¦ç”Ÿã€‚<|im_end|>
<|im_start|>assistant


å¼€å§‹ç”Ÿæˆå›å¤ï¼ˆæ€ç»´é“¾æ¨¡å¼ - vLLMï¼‰...
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.38s/it, est. speed input: 3.52 toks/s, output: 34.86 toks/s]Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.38s/it, est. speed input: 3.52 toks/s, output: 34.86 toks/s]
AIå›å¤ï¼ˆæ€ç»´é“¾æ¨¡å¼ - vLLMï¼‰:
============================================================
<think>
å¥½çš„ï¼Œç”¨æˆ·æ˜¯é‚±é”¡é¹è€å¸ˆçš„å­¦ç”Ÿï¼Œè¿™å¯èƒ½æ„å‘³ç€ä»–å¯èƒ½å¯¹è‡ªç„¶è¯­è¨€å¤„ç†ã€æ·±åº¦å­¦ä¹ ç­‰é¢†åŸŸæœ‰æµ“åšå…´è¶£ã€‚é‚±é”¡é¹è€å¸ˆæ˜¯é˜¿é‡Œå·´å·´è¾¾æ‘©é™¢çš„ç ”ç©¶å‘˜ï¼Œä¸“æ³¨äºè‡ªç„¶è¯­è¨€å¤„ç†å’Œæœºå™¨å­¦ä¹ ï¼Œæ‰€ä»¥ç”¨æˆ·å¯èƒ½å¯¹è¿™äº›é¢†åŸŸæœ‰æ·±å…¥çš„ç ”ç©¶æˆ–å­¦ä¹ ç»å†ã€‚

åœ¨å›åº”æ—¶ï¼Œéœ€è¦ä½“ç°å‡ºå¯¹å­¦æœ¯å’Œç ”ç©¶çš„çƒ­æƒ…ï¼ŒåŒæ—¶ä¿æŒå‹å¥½å’Œå¼€æ”¾çš„æ€åº¦ã€‚å¯ä»¥é€šè¿‡è¯¢é—®ç”¨æˆ·ç›®å‰çš„ç ”ç©¶æ–¹å‘æˆ–æ„Ÿå…´è¶£çš„è¯¾é¢˜ï¼Œæ¥ä¿ƒè¿›æ›´æ·±å…¥çš„äº¤æµã€‚åŒæ—¶ï¼Œè€ƒè™‘åˆ°ç”¨æˆ·å¯èƒ½æ­£åœ¨å­¦ä¹ æˆ–ä»äº‹ç›¸å…³é¢†åŸŸï¼Œå¯ä»¥åˆ†äº«ä¸€äº›æœ€æ–°çš„ç ”ç©¶æˆæœæˆ–èµ„æºï¼Œå¸®åŠ©ä»–æ‹“å±•çŸ¥è¯†é¢ã€‚

å¦å¤–ï¼Œç”¨æˆ·å¯èƒ½å¸Œæœ›å¾—åˆ°ä¸€äº›æŒ‡å¯¼æˆ–å»ºè®®ï¼Œå› æ­¤åœ¨å›åº”ä¸­å¯ä»¥è¡¨è¾¾æ„¿æ„æä¾›å¸®åŠ©çš„æ„æ„¿ï¼Œä½†ä¹Ÿè¦æ³¨æ„ä¸è¦è¿‡äºç›´æ¥ï¼Œä¿æŒé€‚åº¦çš„è·ç¦»æ„Ÿã€‚é€šè¿‡è¯¢é—®å…·ä½“çš„é—®é¢˜æˆ–éœ€æ±‚ï¼Œå¯ä»¥æ›´å¥½åœ°äº†è§£ç”¨æˆ·çš„å®é™…éœ€è¦ï¼Œä»è€Œæä¾›æ›´æœ‰é’ˆå¯¹æ€§çš„æ”¯æŒã€‚

è¿˜è¦æ³¨æ„ä¿æŒè¯­è¨€çš„è‡ªç„¶å’Œå£è¯­åŒ–ï¼Œé¿å…è¿‡äºæ­£å¼æˆ–ç”Ÿç¡¬ã€‚å¯ä»¥é€‚å½“ä½¿ç”¨ä¸€äº›è¡¨æƒ…ç¬¦å·æˆ–è½»æ¾çš„è¯­æ°”ï¼Œè®©å¯¹è¯æ›´åŠ äº²åˆ‡ã€‚åŒæ—¶ï¼Œä¿æŒå›ç­”çš„ç®€æ´æ€§ï¼Œé¿å…ä¿¡æ¯è¿‡è½½ï¼Œè®©ç”¨æˆ·èƒ½å¤Ÿè½»æ¾ç†è§£å’Œå›åº”ã€‚

æœ€åï¼Œç¡®ä¿å›åº”ä¸­åŒ…å«é¼“åŠ±å’Œç§¯æçš„æ€åº¦ï¼Œè®©ç”¨æˆ·æ„Ÿå—åˆ°æ”¯æŒå’Œè®¤å¯ï¼Œè¿™æœ‰åŠ©äºå»ºç«‹è‰¯å¥½çš„äº’åŠ¨æ°›å›´ã€‚åŒæ—¶ï¼Œä¿æŒå¼€æ”¾æ€§ï¼Œè®©ç”¨æˆ·æœ‰æ›´å¤šæœºä¼šè¿›ä¸€æ­¥äº¤æµå’Œåˆ†äº«ã€‚
</think>

ä½ å¥½ï¼å¾ˆé«˜å…´è®¤è¯†ä½ ï¼é‚±é”¡é¹è€å¸ˆåœ¨è‡ªç„¶è¯­è¨€å¤„ç†å’Œæ·±åº¦å­¦ä¹ é¢†åŸŸç¡®å®æœ‰å¾ˆå¤šç²¾å½©çš„æˆæœï¼Œæ¯”å¦‚ä»–åœ¨Transformerã€å¤§æ¨¡å‹ç­‰æ–¹é¢çš„ç ”ç©¶ã€‚ä½ ç›®å‰åœ¨ç ”ç©¶ä»€ä¹ˆæ–¹å‘å‘¢ï¼Ÿå¦‚æœæœ‰ä»€ä¹ˆé—®é¢˜æˆ–è€…æƒ³è®¨è®ºçš„å†…å®¹ï¼Œéšæ—¶å¯ä»¥å‘Šè¯‰æˆ‘ï¼ ğŸ˜Š
============================================================
æ¨ç†è€—æ—¶: 9.38 ç§’

=== æµ‹è¯•å®Œæˆ ===
[rank0]:[W707 10:58:33.457940169 ProcessGroupNCCL.cpp:1250] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
