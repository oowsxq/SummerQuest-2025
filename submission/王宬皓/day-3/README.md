> https://github.com/henrywch/SummerQuest-2025.git

### Day 3 HW

#### Adding Special Tokens

- *hw3_1.py* essential codes

```python

# 1. åŠ è½½ tokenizer
tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)

# 2. å®šä¹‰ç‰¹æ®Š tokens
new_tokens = ["<|AGENT|>", "<|EDIT|>"] # TODO

# 3. æ·»åŠ ç‰¹æ®Š tokens
tokenizer.add_special_tokens({"additional_special_tokens": new_tokens}) # TODO

# 4. ä¿å­˜ä¿®æ”¹åçš„tokenizeråˆ°æœ¬åœ°
tokenizer.save_pretrained(TOKENIZER_SAVE_PATH) # TODO

# 5. è¯»å–åŸå§‹çš„ Query&Output
with open(INPUT_JSON, "r", encoding="utf-8") as f:
    tasks = json.load(f)

# 6. åˆå¹¶ Query å’Œ Outputï¼Œç”Ÿæˆè¾“å‡ºè®°å½•
records = {
    "special_tokens": [
        {
            "token": token,
            "id": tokenizer.convert_tokens_to_ids(token)
        } for token in new_tokens
    ],
    "tasks": []
}

for item in tasks:
    # åˆå¹¶å­—æ®µ
    merged_text = item["Query"].strip() + "\n" + item["Output"].strip()
    # ç¼–ç å¹¶è·å– token IDs
    ids = tokenizer.encode(merged_text, add_special_tokens=True) # TODO
    # è§£ç éªŒè¯
    decoded = tokenizer.decode(ids) # TODO
    records["tasks"].append({
        "text": merged_text,
        "token_ids": ids,
        "decoded_text": decoded
    })

# 7. ç­”æ¡ˆå†™å…¥ JSON
with open(OUTPUT_JSON, "w", encoding="utf-8") as f:
    json.dump(records, f, ensure_ascii=False, indent=2)

```

#### Agents with Special Tokens

- *hw3_2.py* essential codes

Apply In-Context Learning.

```python

def generate_prompt(query: str) -> str:
    """
    ä¸ºå•ä¸ªæŸ¥è¯¢ç”Ÿæˆprompt
    """
    # TODO
    # The system content should guide the model to use the special tokens and tools.
    # We want it to act as a Github Copilot, using AGENT mode (python then editor) for debugging
    # and EDIT mode (editor only) for direct code modification/merging.
    system_content = (
        "ä½ æ˜¯ä¸€ä¸ªGithub Copilotï¼Œèƒ½å¤Ÿå¸®åŠ©ç”¨æˆ·è°ƒè¯•ã€åˆ†æå’Œä¿®æ”¹ä»£ç ã€‚è¯·æ ¹æ®ç”¨æˆ·éœ€æ±‚é€‰æ‹©ä»¥ä¸‹ä¸¤ç§æ¨¡å¼è¿›è¡Œå“åº”ï¼š\n"
        "1. **ä»£ç†æ¨¡å¼ (<|AGENT|>)**: å½“ç”¨æˆ·éœ€è¦è°ƒè¯•æˆ–åˆ†æä»£ç é—®é¢˜æ—¶ï¼Œå…ˆä½¿ç”¨ `python` å·¥å…·æ‰§è¡Œä»£ç è¿›è¡Œè°ƒè¯•å’Œåˆ†æï¼Œç„¶åä½¿ç”¨ `editor` å·¥å…·è¿›è¡Œä¿®æ”¹ã€‚ä»£ç†æ¨¡å¼çš„è¾“å‡ºåº”è¯¥ä»¥ `<|AGENT|>` å¼€å¤´ã€‚\n"
        "2. **ç¼–è¾‘æ¨¡å¼ (<|EDIT|>)**: å½“ç”¨æˆ·éœ€è¦ç›´æ¥ä¿®æ”¹ã€åˆå¹¶ä»£ç æˆ–è¿›è¡Œä»£ç é‡æ„æ—¶ï¼Œç›´æ¥ä½¿ç”¨ `editor` å·¥å…·ã€‚ç¼–è¾‘æ¨¡å¼çš„è¾“å‡ºåº”è¯¥ä»¥ `<|EDIT|>` å¼€å¤´ã€‚\n"
        "è¯·ç¡®ä¿ä½ çš„è¾“å‡ºä¸¥æ ¼éµå¾ªæ‰€é€‰æ¨¡å¼çš„æ ¼å¼ï¼Œå¹¶è°ƒç”¨ç›¸åº”çš„å·¥å…·ã€‚"
    )

    messages = [
        {"role": "system", "content": system_content},
        {"role": "user", "content": query}
    ]
    
    text = tokenizer.apply_chat_template(
        messages, 
        tools=tools, 
        tokenize=False, 
        add_generation_prompt=True
    ) # TODO
    
    return text

```

#### Checking Special Tokenized Outputs

- running *output_checker.py* 

```python
ğŸš€ å¼€å§‹æ£€æŸ¥æ–‡ä»¶...
ğŸ“ æ–‡ä»¶è·¯å¾„: hw3_2.json

============================================================
ğŸ“‹ hw3_checker.py æ£€æŸ¥ç»“æœ
============================================================
ğŸ“Š æ€»ä½“ç»Ÿè®¡:
   æ€»é¡¹ç›®æ•°: 10
   âœ… é€šè¿‡: 10
   âŒ å¤±è´¥: 0
   ğŸ“ˆ é€šè¿‡ç‡: 100.0%

ğŸ“ è¯¦ç»†æ£€æŸ¥ç»“æœ:
   é¡¹ç›® 0: âœ… é€šè¿‡æ‰€æœ‰æ£€æŸ¥
   é¡¹ç›® 1: âœ… é€šè¿‡æ‰€æœ‰æ£€æŸ¥
   é¡¹ç›® 2: âœ… é€šè¿‡æ‰€æœ‰æ£€æŸ¥
   é¡¹ç›® 3: âœ… é€šè¿‡æ‰€æœ‰æ£€æŸ¥
   é¡¹ç›® 4: âœ… é€šè¿‡æ‰€æœ‰æ£€æŸ¥
   é¡¹ç›® 5: âœ… é€šè¿‡æ‰€æœ‰æ£€æŸ¥
   é¡¹ç›® 6: âœ… é€šè¿‡æ‰€æœ‰æ£€æŸ¥
   é¡¹ç›® 7: âœ… é€šè¿‡æ‰€æœ‰æ£€æŸ¥
   é¡¹ç›® 8: âœ… é€šè¿‡æ‰€æœ‰æ£€æŸ¥
   é¡¹ç›® 9: âœ… é€šè¿‡æ‰€æœ‰æ£€æŸ¥
============================================================
```