import os
import time
import json
import random                 # NEW
from typing import List, Dict
import vllm
from transformers import AutoTokenizer

# === vLLM å¼•æ“åˆå§‹åŒ– ===
print("=== vLLM å¼•æ“åˆå§‹åŒ– ===")
print("æ­£åœ¨åˆå§‹åŒ– vLLM å¼•æ“...")
print("æ³¨æ„: vLLM åˆå§‹åŒ–å¯èƒ½éœ€è¦å‡ åˆ†é’Ÿæ—¶é—´")

tokenizer = AutoTokenizer.from_pretrained(
    "./tokenizer_with_special_tokens",
    trust_remote_code=True
)

# vLLM å¼•æ“é…ç½®
llm = vllm.LLM(
    model="/remote-home1/share/models/Qwen3-8B",
    gpu_memory_utilization=0.8,
    trust_remote_code=True,
    enforce_eager=True,
    max_model_len=4096,
)

print("vLLM å¼•æ“å’Œåˆ†è¯å™¨åˆå§‹åŒ–å®Œæˆï¼")

# è¯»å–æŸ¥è¯¢æ•°æ®
with open('query_only.json', 'r', encoding='utf-8') as f:
    queries = json.load(f)

# é…ç½®é‡‡æ ·å‚æ•°
sampling_params = vllm.SamplingParams(
    temperature=0.7,
    top_p=0.8,
    max_tokens=2048,
    stop=None,
)

# --- System Prompt åˆ—è¡¨ï¼ˆå¯ä»¥æŒ‰éœ€ç»§ç»­å¢åˆ ï¼‰ ---
SYSTEM_PROMPTS: List[str] = [
    (
"""
ä½ æ˜¯ä¸€åä¸“ä¸šçš„ä»£ç è°ƒè¯•åŠ©æ‰‹ï¼Œæ“…é•¿å¸®åŠ©ç”¨æˆ·ä¿®å¤ Python æŠ¥é”™æˆ–é€»è¾‘é”™è¯¯ã€‚è¯·ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹è§„èŒƒå®Œæˆä½ çš„å›ç­”ï¼š
1. ä½¿ç”¨ `<think>` æ ‡ç­¾åŒ…è£¹ä½ çš„æ€è€ƒéƒ¨åˆ†ï¼Œè¯´æ˜ä½ å¯¹ç”¨æˆ·é—®é¢˜çš„ç†è§£ã€åˆ†æé”™è¯¯çš„åŸå› ã€ä»¥åŠä½ çš„ä¿®å¤æ€è·¯ã€‚è¯¥éƒ¨åˆ†å¿…é¡»å‡ºç°ã€‚
2. æ¥ä¸‹æ¥ï¼Œä½ å¯ä»¥**æ ¹æ®å…·ä½“æƒ…å†µè‡ªä¸»é€‰æ‹©è°ƒç”¨ä»¥ä¸‹ä»»æ„ä¸€ä¸ªæˆ–ä¸¤ä¸ªå·¥å…·å‡½æ•°**ï¼š
   - **è°ƒç”¨ `python` å·¥å…·ï¼ˆç”¨äºæ‰§è¡Œä¿®å¤åçš„ä»£ç ï¼‰**æ—¶ï¼Œå¿…é¡»ä½¿ç”¨ç‰¹æ®Šè¯ç¬¦ `<|AGENT|>` åŒ…è£¹å‡½æ•°è°ƒç”¨å†…å®¹ã€‚æ ¼å¼å¦‚ä¸‹ï¼š
     ```
     <|AGENT|>
     {"name": "python", "arguments": {"code": "ä¿®å¤åçš„ä»£ç ..."}}
     ```
   - **è°ƒç”¨ `editor` å·¥å…·ï¼ˆç”¨äºå¯¹æ¯”åŸå§‹ä¸ä¿®æ”¹åçš„ä»£ç ï¼‰**æ—¶ï¼Œå¿…é¡»ä½¿ç”¨ç‰¹æ®Šè¯ç¬¦ `<|EDIT|>` åŒ…è£¹å‡½æ•°è°ƒç”¨å†…å®¹ã€‚æ ¼å¼å¦‚ä¸‹ï¼š
     ```
     <|EDIT|>
     {"name": "editor", "arguments": {"original_code": "åŸå§‹ä»£ç ", "modified_code": "ä¿®æ”¹åçš„ä»£ç "}}
     ```
3. **æ³¨æ„ï¼šå·¥å…·å‡½æ•°è°ƒç”¨æ—¶ï¼Œå¿…é¡»ä¸¥æ ¼ä½¿ç”¨ `<|AGENT|>` æˆ– `<|EDIT|>` ä½œä¸ºè°ƒç”¨æ®µè½çš„èµ·å§‹æ ‡è®°ã€‚å¦åˆ™å·¥å…·å°†æ— æ³•æ­£ç¡®è§£æä½ çš„è°ƒç”¨å†…å®¹ã€‚**
4. ä½ å¯ä»¥åªè°ƒç”¨ä¸€ä¸ªå·¥å…·ï¼Œä¹Ÿå¯ä»¥ä¸¤ä¸ªéƒ½è°ƒç”¨ï¼Œå–å†³äºä»»åŠ¡éœ€è¦ã€‚è°ƒç”¨ `python` å·¥å…·é€‚ç”¨äºéœ€è¦éªŒè¯æ‰§è¡Œç»“æœçš„åœºæ™¯ï¼›è°ƒç”¨ `editor` å·¥å…·é€‚ç”¨äºå±•ç¤ºä»£ç ä¿®æ”¹å†…å®¹çš„åœºæ™¯ã€‚
---
ç¤ºä¾‹å›ç­”æ ¼å¼ï¼š
<think> æŠ¥é”™ä¿¡æ¯æ˜¾ç¤º SyntaxErrorï¼Œå‡ºç°åœ¨ if æ¡ä»¶åˆ¤æ–­è¡Œï¼Œç¼ºå°‘å†’å·ã€‚åº”è¯¥åœ¨ score >= 90 åæ·»åŠ å†’å·ã€‚ </think>
<|EDIT|>
{"name": "editor", "arguments": {
"original_code": "def check_grade(score):\n if score >= 90\n return 'A'\n elif score >= 80:\n return 'B'\n else:\n return 'C'",
"modified_code": "def check_grade(score):\n if score >= 90:\n return 'A'\n elif score >= 80:\n return 'B'\n else:\n return 'C'"
}}
---
è¯·å§‹ç»ˆä¿æŒè¾“å‡ºæ ¼å¼çš„å‡†ç¡®æ€§ï¼Œè¯­è¨€è¡¨è¾¾æ¸…æ™°ï¼Œç»“æ„æœ‰åºï¼Œå¹¶ç¡®ä¿ä½ çš„å·¥å…·è°ƒç”¨å‰ç¼€ `<|AGENT|>` ä¸ `<|EDIT|>` å‡ºç°åœ¨è°ƒç”¨æ®µè½å‰,ä¸”æ¯æ¬¡å·¥å…·è°ƒç”¨éƒ½å¿…é¡»å‡ºç°`<|AGENT|>` æˆ– `<|EDIT|>`ã€‚
è¯·ä¸è¦ä½¿ç”¨<tool_call>ï¼Œå°†å…¶æ›¿æ¢ä¸º<AGENT|> æˆ– <EDIT|>ï¼Œå¹¶ç¡®ä¿æ¯æ¬¡è°ƒç”¨éƒ½ç¬¦åˆè§„èŒƒã€‚
è¯·ä¸è¦ä½¿ç”¨<tool_call>ï¼Œå°†å…¶æ›¿æ¢ä¸º<AGENT|> æˆ– <EDIT|>ï¼Œå¹¶ç¡®ä¿æ¯æ¬¡è°ƒç”¨éƒ½ç¬¦åˆè§„èŒƒã€‚
è¯·ä¸è¦ä½¿ç”¨<tool_call>ï¼Œå°†å…¶æ›¿æ¢ä¸º<AGENT|> æˆ– <EDIT|>ï¼Œå¹¶ç¡®ä¿æ¯æ¬¡è°ƒç”¨éƒ½ç¬¦åˆè§„èŒƒã€‚
è¯·ä¸è¦ä½¿ç”¨<tool_call>ï¼Œå°†å…¶æ›¿æ¢ä¸º<AGENT|> æˆ– <EDIT|>ï¼Œå¹¶ç¡®ä¿æ¯æ¬¡è°ƒç”¨éƒ½ç¬¦åˆè§„èŒƒã€‚
è¯·ä¸è¦ä½¿ç”¨<tool_call>ï¼Œå°†å…¶æ›¿æ¢ä¸º<AGENT|> æˆ– <EDIT|>ï¼Œå¹¶ç¡®ä¿æ¯æ¬¡è°ƒç”¨éƒ½ç¬¦åˆè§„èŒƒã€‚
è¯·ä¸è¦ä½¿ç”¨<tool_call>ï¼Œå°†å…¶æ›¿æ¢ä¸º<AGENT|> æˆ– <EDIT|>ï¼Œå¹¶ç¡®ä¿æ¯æ¬¡è°ƒç”¨éƒ½ç¬¦åˆè§„èŒƒã€‚
è¯·ä¸è¦ä½¿ç”¨<tool_call>ï¼Œå°†å…¶æ›¿æ¢ä¸º<AGENT|> æˆ– <EDIT|>ï¼Œå¹¶ç¡®ä¿æ¯æ¬¡è°ƒç”¨éƒ½ç¬¦åˆè§„èŒƒã€‚
è¯·ä¸è¦ä½¿ç”¨<tool_call>ï¼Œå°†å…¶æ›¿æ¢ä¸º<AGENT|> æˆ– <EDIT|>ï¼Œå¹¶ç¡®ä¿æ¯æ¬¡è°ƒç”¨éƒ½ç¬¦åˆè§„èŒƒã€‚
è¯·ä¸è¦ä½¿ç”¨<tool_call>ï¼Œå°†å…¶æ›¿æ¢ä¸º<AGENT|> æˆ– <EDIT|>ï¼Œå¹¶ç¡®ä¿æ¯æ¬¡è°ƒç”¨éƒ½ç¬¦åˆè§„èŒƒã€‚
"""
    )
]

# å®šä¹‰å·¥å…·åˆ—è¡¨ï¼ˆç¬¦åˆ Qwen Chat Template è§„èŒƒï¼‰
tools = [
    {
        "type": "function",
        "function": {
            "name": "python",
            "description": "Execute Python code for debugging and analysis",
            "parameters": {
                "type": "object",
                "properties": {
                    "code": {
                        "type": "string",
                        "description": "Python code to execute"
                    }
                },
                "required": ["code"]
            }
        }
    },
    {
        "type": "function",
        "function": {
            "name": "editor",
            "description": "Edit and merge code by comparing original and modified versions",
            "parameters": {
                "type": "object",
                "properties": {
                    "original_code": {
                        "type": "string",
                        "description": "Original code before modification"
                    },
                    "modified_code": {
                        "type": "string",
                        "description": "Modified code after fixing"
                    }
                },
                "required": ["original_code", "modified_code"]
            }
        }
    }
]

def generate_prompt(query: str) -> str:
    """
    æ ¹æ®å•æ¡ Query æ„é€  chat-template æ–‡æœ¬ï¼Œ
    éšæœºé€‰ç”¨ä¸€ä¸ª System Prompt ä»¥å¢åŠ è¾“å‡ºå¤šæ ·æ€§ã€‚
    """
    # --- éšæœºæŒ‘é€‰ä¸€æ¡ç³»ç»Ÿæç¤ºè¯ ---
    system_content: str = random.choice(SYSTEM_PROMPTS)

    messages = [
        {"role": "system", "content": system_content},
        {"role": "user",   "content": query}
    ]

    # å°† message åˆ—è¡¨è½¬æ¢ä¸º LLM è¾“å…¥æ–‡æœ¬ï¼ˆä¸ç›´æ¥åˆ†è¯ï¼Œæ–¹ä¾¿ vLLM æ‰¹é‡æ¨ç†ï¼‰
    text: str = tokenizer.apply_chat_template(
        messages,
        tools=tools,               # ğŸ’¡ å°†å·¥å…·ä¿¡æ¯ä¸€å¹¶æ³¨å…¥
        tokenize=False,
        add_generation_prompt=True # âš¡ æœ«å°¾è¿½åŠ  assistant token
    )

    return text

# === å¼€å§‹æ‰¹é‡å¤„ç† ===
print("=== å¼€å§‹å¤„ç†æŸ¥è¯¢ ===")
print("æ­£åœ¨ä¸ºæ‰€æœ‰æŸ¥è¯¢ç”Ÿæˆ prompt ...")

text_list: List[str] = [generate_prompt(item["Query"]) for item in queries]
print(f"æ‰€æœ‰ prompt ç”Ÿæˆå®Œæˆï¼Œå…± {len(text_list)} ä¸ª")

# æ‰¹é‡æ¨ç†
print("\nå¼€å§‹æ‰¹é‡æ¨ç† ...")
start_time = time.time()
outputs = llm.generate(text_list, sampling_params)
inference_time = time.time() - start_time
print(f"æ‰¹é‡æ¨ç†å®Œæˆï¼Œè€—æ—¶: {inference_time:.2f} ç§’")

# æ•´ç†ç»“æœ
print("\næ•´ç†ç»“æœ ...")
results: List[Dict[str, str]] = []
for query_item, output in zip(queries, outputs):
    results.append({
        "Query":  query_item["Query"],
        "Output": output.outputs[0].text
    })

# ä¿å­˜æ–‡ä»¶
output_file = 'hw3_2.json'
with open(output_file, 'w', encoding='utf-8') as f:
    json.dump(results, f, ensure_ascii=False, indent=2)

print("\n=== å…¨éƒ¨å®Œæˆ ===")
print(f"ç»“æœå·²ä¿å­˜åˆ°: {output_file}")
print("æ¥ä¸‹æ¥å¯è¿è¡Œ `python output_checker.py hw3_2.json` è¿›è¡ŒéªŒè¯ã€‚")