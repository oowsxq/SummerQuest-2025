# 答题卡

## 1 并行策略与张量 shape

### 1.1

#### 1.1.1
由于是列并行，所以 $\R^{d\times 4d}$ 的权重矩阵被拆分为 $P=4$ 个子矩阵，每个 rank 上的子矩阵的 shape 为 $(d, 4d/P)=(d,d)=(1024,1024)$

#### 1.1.2
每个 rank 上的张量 $X$ 均为完整拷贝， shape 为 $(B,S,d)=(8,128,1024)$

#### 1.1.3
令 $W^{(i)}$ 代表第 $i$ 个 rank，有 $Y_i=X W_1^{(i)} \in \R^{B \times S \times 4d/P}$，因此 $Y_i$ 的 shape 为 $(B, S, d)=(8, 128, 1024)$
完整的 $Y$ 可以由各 rank $Y_i$ concatenate 得到，即 `Y = concatenate([Y1, Y2, Y3, Y4])`


### 1.2


#### 1.2.1
$W_2$ 按行切分后，每个子矩阵 $W_2^{(i)} \in \R^{4d/P \times d}$，因此 shape 为 $(1024, 1024)$

#### 1.2.2
$Z=YW_2$ 中若 $W_2$ 按行切分，则 $Y$ 应当按列切分，因此 $Y_i \in \R^{BS \times 4d/P}$，因此每个 rank 接受的输入 $Y_i$ 的 shape 为 $(8, 128, 1024)$

#### 1.2.3
$Z_i=Y_i W_2^{(i)} \in \R^{BS\times d}$，有 $Z_i$ 的 shape 为 $(8, 128, 1024)$
完整 $Z$ 由所有 rank 的 $Z_i$ 加和得到，即 `Z=sum([Z_1, Z_2, Z_3, Z_4])`

## 2 通信分析

### 2.1

#### 2.1.1
前向过程中由于权重采用列切分， $XW_1 = [X W_1^{(1)}, XW_1^{(2)}]$ 过程中无需权重间通信。

#### 2.1.2
对于每个 rank 有 $\frac{\partial L}{\partial X^{(i)}}=\frac{\partial L}{\partial Y^{(i)}}\frac{\partial Y^{(i)}}{\partial W_1^{(i)}}$，$X^{(i)}=X$，有 $\frac{\partial L}{\partial X}=\sum_{i=1}^2 \frac{\partial L}{\partial Y^{(i)}}\frac{\partial Y^{(i)}}{\partial W_1^{(i)}}$ 因此需要 all-reduce 合并各 rank 的结果，通信量为 $2\Phi=2 B\times S \times d$ elements

### 2.2

#### 2.2.1
前向过程中需要计算 $Y W_2 = Y \begin{bmatrix} W_2^{(1)} \\ W_2^{(2)} \end{bmatrix}=YW_2^{(1)} + W_2^{(2)}$，因此需要一次 all-reduce 合并各 rank 的结果。通信量为 $2\Phi=2 B\times S \times d$ elements

#### 2.2.2
反向传播过程中，对于 linear2 来说，$\partial L / \partial X$ 实际上计算的是 $\partial L / \partial Y$ 的部分。对于每个分块来说，只需要计算 $\partial L / \partial Y^{(i)}$，这个过程中两个分块独立计算，无需通信。

# 3 如果两层都使用 Row Parallel，会产生哪些额外通信？两层都使用 Column Parallel 会带来什么问题？
如果两层都用 Row Parallel，则：
1. 第一层 forward 过程需要一次额外的 all-reduce 通信
2. 第二层 forward 不变，无需通信
3. 第二层 backward 不变，需要一次 all-reduce 通信
4. 第一层 backward 需要一次 all-reduce 通信，合并各 rank 的结果

综上：会多一次 all-reduce 通信


如果两层都是用 Column Parallel，则：
1. 第一层 forward 过程不变，无需通信
2. 第二层 forward 过程需要计算 $[Y_1,Y_2][W_2^{(1)}, W_2^{(2)}]$，需要一次 all-gather 在各 rank 上获得完整的 $Y$
3. 第二层 backward 时需要一次 all-reduce 合并各 rank 的 $\partial L / \partial W_2^{(i)}$
4. 第一层 backward 不变，需要一次 all-reduce 通信

综上：会多义词 all-gather 通信