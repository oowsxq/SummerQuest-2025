
# conda environments:
#
base                   /remote-home1/qywu/miniconda3
audioldm_train         /remote-home1/qywu/miniconda3/envs/audioldm_train

/remote-home1/moss/miniconda3/envs/verl/bin/python
=== ç¯å¢ƒä¿¡æ¯æ£€æŸ¥ ===

INFO 07-06 13:33:14 [__init__.py:239] Automatically detected platform cuda.
vLLM ç‰ˆæœ¬: 0.8.5

=== vLLM å¼•æ“åˆå§‹åŒ– ===
æ­£åœ¨åˆå§‹åŒ– vLLM å¼•æ“...
æ³¨æ„: vLLM åˆå§‹åŒ–å¯èƒ½éœ€è¦å‡ åˆ†é’Ÿæ—¶é—´
æ­£åœ¨åŠ è½½åˆ†è¯å™¨...
vLLM å¼•æ“å’Œåˆ†è¯å™¨åˆå§‹åŒ–å®Œæˆï¼

INFO 07-06 13:33:24 [config.py:717] This model supports multiple tasks: {'reward', 'generate', 'classify', 'embed', 'score'}. Defaulting to 'generate'.
INFO 07-06 13:33:24 [config.py:2003] Chunked prefill is enabled with max_num_batched_tokens=8192.
WARNING 07-06 13:33:24 [cuda.py:93] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used
INFO 07-06 13:33:25 [core.py:58] Initializing a V1 LLM engine (v0.8.5) with config: model='/remote-home1/share/models/Qwen3-8B', speculative_config=None, tokenizer='/remote-home1/share/models/Qwen3-8B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='auto', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=/remote-home1/share/models/Qwen3-8B, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=False, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[],"max_capture_size":0}
WARNING 07-06 13:33:26 [utils.py:2522] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x14e7fd29dac0>
INFO 07-06 13:33:26 [parallel_state.py:1004] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0
INFO 07-06 13:33:26 [cuda.py:221] Using Flash Attention backend on V1 engine.
WARNING 07-06 13:33:26 [topk_topp_sampler.py:69] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
INFO 07-06 13:33:26 [gpu_model_runner.py:1329] Starting to load model /remote-home1/share/models/Qwen3-8B...
INFO 07-06 13:33:37 [loader.py:458] Loading weights took 10.75 seconds
INFO 07-06 13:33:37 [gpu_model_runner.py:1347] Model loading took 15.2683 GiB and 10.952767 seconds
INFO 07-06 13:33:40 [kv_cache_utils.py:634] GPU KV cache size: 13,280 tokens
INFO 07-06 13:33:40 [kv_cache_utils.py:637] Maximum concurrency for 4,096 tokens per request: 3.24x
INFO 07-06 13:33:40 [core.py:159] init engine (profile, create kv cache, warmup model) took 2.27 seconds
INFO 07-06 13:33:40 [core_client.py:439] Core engine process 0 ready.
=== æµ‹è¯•1: æ™®é€šå¯¹è¯æ¨¡å¼ (vLLM) ===
è¾“å…¥æ–‡æœ¬: <|im_start|>system
ä½ æ˜¯ä¸€ä¸ªä¼šæ·±æ€ç†Ÿè™‘çš„AIåŠ©æ‰‹ã€‚<|im_end|>
<|im_start|>user
ä½ å¥½ï¼Œæˆ‘æ˜¯é‚±é”¡é¹è€å¸ˆçš„å­¦ç”Ÿã€‚<|im_end|>
<|im_start|>assistant
<think>

</think>



å¼€å§‹ç”Ÿæˆå›å¤ (vLLM)...
AIå›å¤ï¼ˆæ™®é€šæ¨¡å¼ - vLLMï¼‰:
============================================================
ä½ å¥½ï¼Œå¾ˆé«˜å…´è®¤è¯†ä½ ï¼æˆ‘æ˜¯é‚±é”¡é¹è€å¸ˆçš„å­¦ç”Ÿï¼Œå¾ˆé«˜å…´èƒ½å’Œä½ äº¤æµã€‚æœ‰ä»€ä¹ˆé—®é¢˜æˆ–è€…æƒ³è®¨è®ºçš„å†…å®¹å—ï¼Ÿæ— è®ºæ˜¯å…³äºå­¦ä¹ ã€ç ”ç©¶ï¼Œè¿˜æ˜¯å…¶ä»–æ–¹é¢ï¼Œæˆ‘éƒ½å¾ˆä¹æ„å’Œä½ åˆ†äº«æˆ‘çš„æƒ³æ³•å’Œè§è§£ã€‚
============================================================
æ¨ç†è€—æ—¶: 1.20 ç§’

==================================================

=== æµ‹è¯•2: æ€ç»´é“¾æ¨ç†æ¨¡å¼ (vLLM) ===
è¾“å…¥æ–‡æœ¬: <|im_start|>system
ä½ æ˜¯ä¸€ä¸ªä¼šæ·±æ€ç†Ÿè™‘çš„AIåŠ©æ‰‹ã€‚<|im_end|>
<|im_start|>user
ä½ å¥½ï¼Œæˆ‘æ˜¯é‚±é”¡é¹è€å¸ˆçš„å­¦ç”Ÿã€‚<|im_end|>
<|im_start|>assistant


å¼€å§‹ç”Ÿæˆå›å¤ï¼ˆæ€ç»´é“¾æ¨¡å¼ - vLLMï¼‰...
AIå›å¤ï¼ˆæ€ç»´é“¾æ¨¡å¼ - vLLMï¼‰:
============================================================
<think>
å¥½çš„ï¼Œç”¨æˆ·æåˆ°ä»–æ˜¯é‚±é”¡é¹è€å¸ˆçš„å­¦ç”Ÿï¼Œè¿™è¯´æ˜ä»–å¯èƒ½åœ¨å­¦æœ¯é¢†åŸŸï¼Œå°¤å…¶æ˜¯è‡ªç„¶è¯­è¨€å¤„ç†æˆ–æœºå™¨å­¦ä¹ æ–¹é¢æœ‰ç ”ç©¶ã€‚é‚±é”¡é¹è€å¸ˆæ˜¯ä¸­å›½è‘—åçš„è‡ªç„¶è¯­è¨€å¤„ç†ä¸“å®¶ï¼Œæ›¾æ‹…ä»»ç™¾åº¦é¦–å¸­ç§‘å­¦å®¶ï¼Œç°åœ¨åœ¨æ¸…åå¤§å­¦ä»»æ•™ã€‚ä»–çš„ç ”ç©¶é¢†åŸŸåŒ…æ‹¬æ·±åº¦å­¦ä¹ ã€è‡ªç„¶è¯­è¨€å¤„ç†ç­‰ï¼Œç‰¹åˆ«æ˜¯åœ¨Transformeræ¨¡å‹å’Œå¤§æ¨¡å‹æ–¹é¢æœ‰é‡è¦è´¡çŒ®ã€‚

ç”¨æˆ·å¯èƒ½å¸Œæœ›å¾—åˆ°å…³äºå­¦æœ¯ç ”ç©¶ã€è®ºæ–‡å†™ä½œã€æ¨¡å‹è®­ç»ƒæˆ–è€…ç›¸å…³æŠ€æœ¯çš„å»ºè®®ã€‚è€ƒè™‘åˆ°é‚±é”¡é¹è€å¸ˆçš„ç ”ç©¶æ–¹å‘ï¼Œç”¨æˆ·å¯èƒ½å¯¹å¤§æ¨¡å‹ã€Transformeræ¶æ„ã€æ¨¡å‹ä¼˜åŒ–ã€æ•°æ®å¤„ç†ç­‰æ–¹é¢æ„Ÿå…´è¶£ã€‚æ­¤å¤–ï¼Œç”¨æˆ·å¯èƒ½æ­£åœ¨å‡†å¤‡è®ºæ–‡æˆ–é¡¹ç›®ï¼Œéœ€è¦æŒ‡å¯¼æˆ–èµ„æºæ¨èã€‚

éœ€è¦è¿›ä¸€æ­¥äº†è§£ç”¨æˆ·çš„å…·ä½“éœ€æ±‚ï¼Œæ¯”å¦‚ä»–ç›®å‰çš„ç ”ç©¶æ–¹å‘ã€é‡åˆ°çš„é—®é¢˜ï¼Œæˆ–è€…å¸Œæœ›è·å¾—å“ªæ–¹é¢çš„å¸®åŠ©ã€‚åŒæ—¶ï¼Œè€ƒè™‘åˆ°ç”¨æˆ·æ˜¯å­¦ç”Ÿï¼Œå¯èƒ½éœ€è¦å­¦ä¹ èµ„æºã€ç ”ç©¶æ–¹æ³•æˆ–èŒä¸šå‘å±•å»ºè®®ã€‚è¦ç¡®ä¿å›ç­”æ—¢ä¸“ä¸šåˆæ˜“äºç†è§£ï¼Œé¿å…ä½¿ç”¨è¿‡äºå¤æ‚çš„æœ¯è¯­ï¼ŒåŒæ—¶æä¾›å®ç”¨çš„å»ºè®®ã€‚

å¦å¤–ï¼Œç”¨æˆ·å¯èƒ½å¸Œæœ›äº†è§£é‚±é”¡é¹è€å¸ˆæœ€æ–°çš„ç ”ç©¶æˆæœæˆ–å­¦æœ¯åŠ¨æ€ï¼Œæˆ–è€…å¦‚ä½•ä¸å¯¼å¸ˆè¿›è¡Œæœ‰æ•ˆæ²Ÿé€šã€‚å› æ­¤ï¼Œå›ç­”ä¸­å¯ä»¥åŒ…å«ä¸€äº›å…³äºå¦‚ä½•ä¸å¯¼å¸ˆäº’åŠ¨ã€å¦‚ä½•é€‰æ‹©ç ”ç©¶è¯¾é¢˜çš„å»ºè®®ï¼Œä»¥åŠæ¨èç›¸å…³çš„å­¦æœ¯èµ„æºæˆ–å·¥å…·ã€‚

éœ€è¦ä¿æŒå›ç­”çš„ç»“æ„æ¸…æ™°ï¼Œåˆ†ç‚¹åˆ—å‡ºå¯èƒ½çš„å»ºè®®ï¼Œå¹¶é¼“åŠ±ç”¨æˆ·æä¾›æ›´å¤šç»†èŠ‚ä»¥ä¾¿ç»™å‡ºæ›´ç²¾å‡†çš„å¸®åŠ©ã€‚åŒæ—¶ï¼Œè¦è¡¨è¾¾å‡ºå¯¹ç”¨æˆ·å­¦æœ¯å‘å±•çš„æ”¯æŒå’Œé¼“åŠ±ï¼Œå»ºç«‹ç§¯æçš„äº’åŠ¨æ°›å›´ã€‚
</think>

ä½ å¥½ï¼å¾ˆé«˜å…´è®¤è¯†ä½ ï¼Œé‚±é”¡é¹è€å¸ˆæ˜¯è‡ªç„¶è¯­è¨€å¤„ç†é¢†åŸŸçš„æ°å‡ºå­¦è€…ï¼Œä»–çš„ç ”ç©¶æ–¹å‘åŒ…æ‹¬æ·±åº¦å­¦ä¹ ã€å¤§æ¨¡å‹ã€è¯­è¨€æ¨¡å‹ç­‰ï¼Œå¯¹å­¦æœ¯ç•Œå’Œå·¥ä¸šç•Œéƒ½æœ‰æ·±è¿œå½±å“ã€‚ä½œä¸ºä»–çš„å­¦ç”Ÿï¼Œä½ ä¸€å®šåœ¨å­¦æœ¯é“è·¯ä¸Šç§¯ç´¯äº†ä¸°å¯Œçš„ç»éªŒã€‚ğŸ˜Š

å¦‚æœä½ æœ‰å…·ä½“é—®é¢˜ï¼Œæ¯”å¦‚ï¼š  
1. **ç ”ç©¶æ–¹å‘**ï¼šæƒ³äº†è§£å¤§æ¨¡å‹ï¼ˆå¦‚Transformerã€LLMï¼‰çš„æœ€æ–°è¿›å±•ï¼Œæˆ–è€…å¦‚ä½•è®¾è®¡é«˜æ•ˆçš„æ¨¡å‹ç»“æ„ï¼Ÿ  
2. **è®ºæ–‡å†™ä½œ**ï¼šéœ€è¦æŒ‡å¯¼å¦‚ä½•æ’°å†™é«˜è´¨é‡çš„å­¦æœ¯è®ºæ–‡ï¼Œæˆ–è€…å¦‚ä½•é€‰æ‹©æœ‰æŒ‘æˆ˜æ€§çš„ç ”ç©¶è¯¾é¢˜ï¼Ÿ  
3. **æŠ€æœ¯é—®é¢˜**ï¼šé‡åˆ°æ¨¡å‹è®­ç»ƒã€æ•°æ®å¤„ç†ã€ä¼˜åŒ–ç­–ç•¥ç­‰æŠ€æœ¯éš¾é¢˜ï¼Ÿ  
4. **èŒä¸šå‘å±•**ï¼šæƒ³äº†è§£å­¦æœ¯ç•Œæˆ–å·¥ä¸šç•Œçš„å‘å±•è·¯å¾„ï¼Œæˆ–è€…å¦‚ä½•è§„åˆ’æœªæ¥çš„ç ”ç©¶æ–¹å‘ï¼Ÿ  

å¯ä»¥å‘Šè¯‰æˆ‘ä½ çš„å…·ä½“éœ€æ±‚ï¼Œæˆ‘ä¼šå°½åŠ›æä¾›å¸®åŠ©ï¼ ğŸš€  
ï¼ˆæ¯”å¦‚ï¼šä½ ç›®å‰çš„ç ”ç©¶è¯¾é¢˜æ˜¯ä»€ä¹ˆï¼Ÿé‡åˆ°äº†å“ªäº›æŒ‘æˆ˜ï¼Ÿï¼‰
============================================================
æ¨ç†è€—æ—¶: 11.79 ç§’

=== æµ‹è¯•å®Œæˆ ===
